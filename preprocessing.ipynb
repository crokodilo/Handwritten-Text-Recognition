{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d201c69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
      "\u001b[K     |█████████████████████████████▏  | 283.2 MB 84.7 MB/s eta 0:00:012"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 63.3 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317143 sha256=85101d804b8ea76270af53e7870db8814059cdb4d22b56b4ce500bcc648d64ce\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-e6v66pzh/wheels/27/3e/a7/888155c6a7f230b13a394f4999b90fdfaed00596c68d3de307\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb712d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/13 17:43:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "507a4a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lines(path = '../datasets/iam'):\n",
    "\n",
    "    with open(os.path.join(path, 'lines.txt'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    description = lines[:23]\n",
    "    data = lines[23:]\n",
    "\n",
    "    rows = []\n",
    "    for line in data:\n",
    "        temp = line.split()\n",
    "        row = temp[:8]\n",
    "        row.extend([' '.join(temp[8:])])\n",
    "        rows.append(row)\n",
    "\n",
    "    cols = ['id', 'result', 'graylevel', 'components', 'x', 'y', 'w', 'h', 'message']\n",
    "    \n",
    "    return rows, cols\n",
    "    \n",
    "def clean_message(m):\n",
    "    m = m.replace('|-|', ' - ')\n",
    "    m = m.replace('|*|', ' * ')\n",
    "    temp0 = m.replace('.|\"', '\"|.')\n",
    "    temp1 = re.sub(r'\"\\|([\\w\\d\\s\\|-]+)\\|\"', r'\"\\1\"', temp0)\n",
    "    temp2 = re.sub(r'([^\\'\\(])\\|([\\w\\d\\\"\\#\\(])', r'\\1 \\2', temp1)\n",
    "    temp3 = re.sub(r'([^\\'\\(\\\"])\\|([\\w\\d\\\"\\#\\(])', r'\\1 \\2', temp2)\n",
    "    temp4 = re.sub(r'^\"\\s', r'\"', temp3)\n",
    "    return temp4.replace('|', '').replace(' \". ', '\". ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4316c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the session\n",
    "conf = SparkConf().\\\n",
    "                set('spark.ui.port', \"4050\").\\\n",
    "                set('spark.executor.memory', '4G').\\\n",
    "                set('spark.driver.memory', '45G').\\\n",
    "                set('spark.driver.maxResultSize', '10G').\\\n",
    "                setAppName(\"PySparkTutorial\").\\\n",
    "                setMaster(\"local[*]\")\n",
    "\n",
    "# Create the context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "path = '../datasets/iam'\n",
    "\n",
    "rows, cols = read_lines()\n",
    "\n",
    "df = spark.createDataFrame(rows, schema = cols)\n",
    "\n",
    "for c in ['components', 'graylevel', 'x', 'y', 'w', 'h']:\n",
    "    df = df.withColumn(c, df[c].cast(IntegerType()))\n",
    "\n",
    "clean_message_udf = udf(lambda m: clean_message(m), StringType())\n",
    "\n",
    "df = df.withColumn(\"clean_message\", clean_message_udf(col(\"message\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d28e323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+---------+----------+---+----+----+---+--------------------+--------------------+\n",
      "|         id|result|graylevel|components|  x|   y|   w|  h|             message|       clean_message|\n",
      "+-----------+------+---------+----------+---+----+----+---+--------------------+--------------------+\n",
      "|a01-000u-00|    ok|      154|        19|408| 746|1661| 89|A|MOVE|to|stop|Mr...|A MOVE to stop Mr...|\n",
      "|a01-000u-01|    ok|      156|        19|395| 932|1850|105|nominating|any|mo...|nominating any mo...|\n",
      "|a01-000u-02|    ok|      157|        16|408|1106|1986|105|is|to|be|made|at|...|is to be made at ...|\n",
      "|a01-000u-03|   err|      156|        23|430|1290|1883| 70|M Ps|tomorrow|.|M...|M Ps tomorrow. Mr...|\n",
      "|a01-000u-04|    ok|      157|        20|395|1474|1830| 94|put|down|a|resolu...|put down a resolu...|\n",
      "+-----------+------+---------+----------+---+----+----+---+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
